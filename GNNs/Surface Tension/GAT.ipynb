{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from dgllife.utils import Meter, EarlyStopping\n",
    "from hyperopt import fmin, tpe\n",
    "from shutil import copyfile\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hyper import init_hyper_space\n",
    "from utils import get_configure, mkdir_p, init_trial_path, \\\n",
    "    split_dataset, collate_molgraphs, load_model, predict, init_featurizer, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "patienceNum = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        if len(smiles) == 1:\n",
    "            # Avoid potential issues with batch normalization\n",
    "            continue\n",
    "\n",
    "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
    "        prediction = predict(args, model, bg)\n",
    "        loss = (loss_criterion(prediction, labels) * (masks != 0).float()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        train_meter.update(prediction, labels, masks)\n",
    "        if batch_id % args['print_every'] == 0:\n",
    "            print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
    "                epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
    "    train_score = np.mean(train_meter.compute_metric(args['metric']))\n",
    "    \n",
    "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric'], train_score))\n",
    "    \n",
    "    if args['metric'] == 'r2':\n",
    "        r2_score = np.mean(train_meter.compute_metric(args['metric']))  # in case of multi-tasks\n",
    "        mae_score = np.mean(train_meter.compute_metric('mae'))  # in case of multi-tasks\n",
    "        rmse_score = np.mean(train_meter.compute_metric('rmse'))  # in case of multi-tasks\n",
    "        return {'r2': r2_score, 'mae': mae_score,'rmse': rmse_score }\n",
    "    else:\n",
    "        roc_score = np.mean(train_meter.compute_metric(args['metric']))  # in case of multi-tasks\n",
    "        prc_score = np.mean(train_meter.compute_metric('prc_auc'))  # in case of multi-tasks\n",
    "        acc_score = np.mean(train_meter.compute_metric('acc'))  # in case of multi-tasks\n",
    "        return {'roc_auc': roc_score, 'prc_auc': prc_score, 'acc': acc_score}\n",
    "\n",
    "def run_an_eval_epoch(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            labels = labels.to(args['device'])\n",
    "            prediction = predict(args, model, bg)\n",
    "            \n",
    "            # loss.cpu()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            eval_meter.update(prediction, labels, masks)\n",
    "            \n",
    "    if args['metric'] == 'r2':\n",
    "        r2_score = np.mean(eval_meter.compute_metric(args['metric']))  # in case of multi-tasks\n",
    "        mae_score = np.mean(eval_meter.compute_metric('mae'))  # in case of multi-tasks\n",
    "        rmse_score = np.mean(eval_meter.compute_metric('rmse'))  # in case of multi-tasks\n",
    "        return {'r2': r2_score, 'mae': mae_score,'rmse': rmse_score }\n",
    "    else:\n",
    "        roc_score = np.mean(eval_meter.compute_metric(args['metric']))  # in case of multi-tasks\n",
    "        prc_score = np.mean(eval_meter.compute_metric('prc_auc'))  # in case of multi-tasks\n",
    "        acc_score = np.mean(eval_meter.compute_metric('acc'))  # in case of multi-tasks\n",
    "        return {'roc_auc': roc_score, 'prc_auc': prc_score, 'acc': acc_score}\n",
    "\n",
    "def main(args, exp_config, train_set, val_set, test_set):\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "        'patience': patienceNum\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "    args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "\n",
    "    loss_criterion = nn.SmoothL1Loss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=args['trial_path'] + '/model.pth',\n",
    "                            metric=args['metric'])\n",
    "\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score[args['metric']], model)\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    \n",
    "    tr_scores = run_an_eval_epoch(args, model, train_loader)\n",
    "    val_scores = run_an_eval_epoch(args, model, val_loader)\n",
    "    te_scores = run_an_eval_epoch(args, model, test_loader)\n",
    "    print({'train': tr_scores, 'valid': val_scores, 'test': te_scores})    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    with open(args['trial_path'] + '/eval.txt', 'w') as f:\n",
    "        f.write('Best val {}: {}\\n'.format(args['metric'], stopper.best_score))\n",
    "        f.write('Test {}: {}\\n'.format(args['metric'], te_scores))\n",
    "\n",
    "    with open(args['trial_path'] + '/configure.json', 'w') as f:\n",
    "        json.dump(exp_config, f, indent=2)\n",
    "\n",
    "    return args['trial_path'], stopper.best_score\n",
    "\n",
    "def bayesian_optimization(args, train_set, val_set, test_set):\n",
    "    # Run grid search\n",
    "    results = []\n",
    "\n",
    "    candidate_hypers = init_hyper_space(args['model'])\n",
    "\n",
    "    def objective(hyperparams):\n",
    "        configure = deepcopy(args)\n",
    "        trial_path, val_metric = main(configure, hyperparams, train_set, val_set, test_set)\n",
    "\n",
    "        if args['metric'] in ['r2']:\n",
    "            # Maximize R2 is equivalent to minimize the negative of it\n",
    "            val_metric_to_minimize = -1 * val_metric\n",
    "        else:\n",
    "            val_metric_to_minimize = val_metric\n",
    "\n",
    "        results.append((trial_path, val_metric_to_minimize))\n",
    "\n",
    "        return val_metric_to_minimize\n",
    "\n",
    "    fmin(objective, candidate_hypers, algo=tpe.suggest, max_evals=args['num_evals'])\n",
    "    results.sort(key=lambda tup: tup[1])\n",
    "    best_trial_path, best_val_metric = results[0]\n",
    "\n",
    "    return best_trial_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-c', '--csv-path', type=str, required=True,\n",
    "                    help='Path to a csv file for loading a dataset')\n",
    "parser.add_argument('-sc', '--smiles-column', type=str, required=True,\n",
    "                    help='Header for the SMILES column in the CSV file')\n",
    "parser.add_argument('-lv', '--log-values', action='store_true', default=False,\n",
    "                    help='Whether to take logarithm of the labels for modeling')\n",
    "parser.add_argument('-t', '--task-names', default=None, type=str,\n",
    "                    help='Header for the tasks to model. If None, we will model '\n",
    "                         'all the columns except for the smiles_column in the CSV file. '\n",
    "                         '(default: None)')\n",
    "parser.add_argument('-s', '--split',\n",
    "                    choices=['scaffold_decompose', 'scaffold_smiles', 'random'],\n",
    "                    default='scaffold_smiles',\n",
    "                    help='Dataset splitting method (default: scaffold_smiles). For scaffold '\n",
    "                         'split based on rdkit.Chem.AllChem.MurckoDecompose, '\n",
    "                         'use scaffold_decompose. For scaffold split based on '\n",
    "                         'rdkit.Chem.Scaffolds.MurckoScaffold.MurckoScaffoldSmiles, '\n",
    "                         'use scaffold_smiles.')\n",
    "parser.add_argument('-sr', '--split-ratio', default='0.8,0.1,0.1', type=str,\n",
    "                    help='Proportion of the dataset to use for training, validation and test '\n",
    "                         '(default: 0.8,0.1,0.1)')\n",
    "parser.add_argument('-me', '--metric', choices=['r2', 'mae', 'rmse'], default='r2',\n",
    "                    help='Metric for evaluation (default: r2)')\n",
    "parser.add_argument('-mo', '--model', choices=['GCN', 'GAT', 'Weave', 'MPNN', 'AttentiveFP',\n",
    "                                               'gin_supervised_contextpred',\n",
    "                                               'gin_supervised_infomax',\n",
    "                                               'gin_supervised_edgepred',\n",
    "                                               'gin_supervised_masking',\n",
    "                                               'NF'],\n",
    "                    default='GCN', help='Model to use (default: GCN)')\n",
    "parser.add_argument('-a', '--atom-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for atoms (default: canonical)')\n",
    "parser.add_argument('-b', '--bond-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for bonds (default: canonical)')\n",
    "parser.add_argument('-n', '--num-epochs', type=int, default=1000,\n",
    "                    help='Maximum number of epochs allowed for training. '\n",
    "                         'We set a large number by default as early stopping '\n",
    "                         'will be performed. (default: 1000)')\n",
    "parser.add_argument('-nw', '--num-workers', type=int, default=0,\n",
    "                    help='Number of processes for data loading (default: 1)')\n",
    "parser.add_argument('-pe', '--print-every', type=int, default=20,\n",
    "                    help='Print the training progress every X mini-batches')\n",
    "parser.add_argument('-p', '--result-path', type=str, default='regression_results',\n",
    "                    help='Path to save training results (default: regression_results)')\n",
    "parser.add_argument('-ne', '--num-evals', type=int, default=None,\n",
    "                    help='Number of trials for hyperparameter search (default: None)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUNum = '0'\n",
    "repetitions = 10\n",
    "args = parser.parse_args(args=['--csv-path','SurFace1881_seed0.csv',\n",
    "                               '--task-names','Surface',\n",
    "                               '--smiles-column','smiles',\n",
    "                               '--result-path','result/SurFace1881_GAT',\n",
    "                               #'--log-values',\n",
    "                               '--num-evals','50',\n",
    "                               '--num-epochs','1000',\n",
    "#                                '--split-ratio',\n",
    "                                '--split','random',                     \n",
    "                               '--metric','r2',\n",
    "                               '--model','GAT',\n",
    "                               '--atom-featurizer-type','attentivefp',\n",
    "                               '--bond-featurizer-type','attentivefp'\n",
    "#                                '--num-workers',\n",
    "#                                '--print-every',\n",
    "                                  ]).__dict__\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def del_file(filepath):\n",
    "    \"\"\"\n",
    "    删除某一目录下的所有文件或文件夹\n",
    "    :param filepath: 路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    del_list = os.listdir(filepath)\n",
    "    for f in del_list:\n",
    "        file_path = os.path.join(filepath, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "            \n",
    "path_data = args['result_path']\n",
    "\n",
    "del_file(path_data)\n",
    "\n",
    "dirs = args['result_path']+'/saved_model'\n",
    "\n",
    "if not os.path.exists(dirs):\n",
    "    os.makedirs(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda:'+ GPUNum)\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "if args['task_names'] is not None:\n",
    "    args['task_names'] = args['task_names'].split(',')\n",
    "    \n",
    "args = init_featurizer(args)\n",
    "df = pd.read_csv(args['csv_path'])\n",
    "mkdir_p(args['result_path'])\n",
    "dataset = load_dataset(args, df)\n",
    "# Whether to take the logarithm of labels for narrowing the range of values\n",
    "if args['log_values']:\n",
    "    dataset.labels = dataset.labels.log()\n",
    "args['n_tasks'] = dataset.n_tasks\n",
    "train_set, val_set, test_set = split_dataset(args, dataset)\n",
    "\n",
    "if args['num_evals'] is not None:\n",
    "    assert args['num_evals'] > 0, 'Expect the number of hyperparameter search trials to ' \\\n",
    "                                  'be greater than 0, got {:d}'.format(args['num_evals'])\n",
    "    print('Start hyperparameter search with Bayesian '\n",
    "          'optimization for {:d} trials'.format(args['num_evals']))\n",
    "    trial_path = bayesian_optimization(args, train_set, val_set, test_set)\n",
    "else:\n",
    "    print('Use the manually specified hyperparameters')\n",
    "    exp_config = get_configure(args['model'])\n",
    "    main(args, exp_config, train_set, val_set, test_set)\n",
    "    trial_path = args['result_path'] + '/1'\n",
    "\n",
    "# Copy final\n",
    "copyfile(trial_path + '/model.pth', args['result_path'] + '/model.pth')\n",
    "copyfile(trial_path + '/configure.json', args['result_path'] + '/configure.json')\n",
    "copyfile(trial_path + '/eval.txt', args['result_path'] + '/eval.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args['result_path']+'/configure.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_res = []\n",
    "val_res = []\n",
    "te_res = []\n",
    "\n",
    "def trainWithHyper (args, exp_config, train_set, val_set, test_set):\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "        'patience': patienceNum\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "#     args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "    \n",
    "    best_model_file = args['result_path']+'/saved_model/%s_bst_%s.pth' % (args['model'], split)\n",
    "\n",
    "    loss_criterion = nn.SmoothL1Loss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    \n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=best_model_file,\n",
    "                            metric=args['metric'])\n",
    "\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score[args['metric']], model)\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    \n",
    "    tr_scores = run_an_eval_epoch(args, model, train_loader)\n",
    "    val_scores = run_an_eval_epoch(args, model, val_loader)\n",
    "    te_scores = run_an_eval_epoch(args, model, test_loader)\n",
    "    \n",
    "    \n",
    "    tr_res.append(tr_scores);\n",
    "    val_res.append(val_scores);\n",
    "    te_res.append(te_scores)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for split in range(1, repetitions + 1):\n",
    "for split in range(1, repetitions + 1):\n",
    "   # raining_data, data_test = train_test_split(my_df, test_size=0.1, random_state=seed)\n",
    "   # data_train, data_val = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "    train_set, val_set, test_set = split_dataset(args,dataset,split)\n",
    "    \n",
    "    trainWithHyper(args, config, train_set, val_set, test_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ 'r2','mae', 'rmse',]\n",
    "tr = [list(item.values()) for item in tr_res]\n",
    "val = [list(item.values()) for item in val_res]\n",
    "te = [list(item.values()) for item in te_res]\n",
    "tr_pd = pd.DataFrame(tr, columns=cols)\n",
    "tr_pd['split'] = range(1, repetitions + 1)\n",
    "tr_pd['set'] = 'train'\n",
    "val_pd = pd.DataFrame(val, columns=cols)\n",
    "val_pd['split'] = range(1, repetitions + 1)\n",
    "val_pd['set'] = 'validation'\n",
    "te_pd = pd.DataFrame(te, columns=cols)\n",
    "te_pd['split'] = range(1, repetitions + 1)\n",
    "te_pd['set'] = 'test'\n",
    "sta_pd = pd.concat([tr_pd, val_pd, te_pd], ignore_index=True)\n",
    "sta_pd['model'] = args['model']\n",
    "sta_pd.to_csv('{}_statistical_results_split10.csv'.format(args['model']), index=False)\n",
    "\n",
    "print('training mean:', np.mean(tr, axis=0), 'training std:', np.std(tr, axis=0))\n",
    "print('validation mean:', np.mean(val, axis=0), 'validation std:', np.std(val, axis=0))\n",
    "print('testing mean:', np.mean(te, axis=0), 'test std:', np.std(te, axis=0))\n",
    "end_time = time.time()\n",
    "print('the total elapsed time is', end_time - start_time, 'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print('the total elapsed time is', (end_time - start_time)/3600, 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the total elapsed time is', (end_time - start_time)/3600, 'H')\n",
    "print(args['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dgllife]",
   "language": "python",
   "name": "conda-env-dgllife-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
